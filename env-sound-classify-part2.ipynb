{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-T5xlz5hJLRM"
   },
   "source": [
    "# env-sound-classify -2\n",
    "\n",
    "## Part 2 - Training Our Deep Learning Model\n",
    "\n",
    "First, start by uploading your x.npy and y.npy files into your Google Drive, and place it into a folder of your choice. You can choose to use your own local machine if you have a powerful GPU that can be used for training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 772,
     "status": "ok",
     "timestamp": 1579508768418,
     "user": {
      "displayName": "Kim Foong Chow",
      "photoUrl": "",
      "userId": "11856562233682856896"
     },
     "user_tz": -480
    },
    "id": "u78Tv02gKEFw",
    "outputId": "b50c2e17-08a7-4d36-dcce-c34751333166"
   },
   "outputs": [],
   "source": [
    "# Run this only if you need to use Google Drive\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 758,
     "status": "ok",
     "timestamp": 1579508914477,
     "user": {
      "displayName": "Kim Foong Chow",
      "photoUrl": "",
      "userId": "11856562233682856896"
     },
     "user_tz": -480
    },
    "id": "Nw8cEkNSRjdZ",
    "outputId": "42a5c2fa-a794-4cb4-ecc8-3c6a945ae579"
   },
   "outputs": [],
   "source": [
    "# Run the following code as it is\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io.wavfile\n",
    "import pandas as pd \n",
    "import os\n",
    "from scipy.fftpack import dct\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import cv2\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Input\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, GlobalAveragePooling2D, GlobalMaxPooling2D\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalAveragePooling1D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "# Make sure that we are using the GPU version on Colab\n",
    "#\n",
    "print (tf.test.gpu_device_name())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ySeRMeOXUa_u"
   },
   "source": [
    "Let's set the folder on Google Drive or your local computer to where the *.npy files are.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CtZ61XyyUlWk"
   },
   "outputs": [],
   "source": [
    "# Set the folder paths. If you are not using Google Drive,\n",
    "# please change the path to point the correct folder containing\n",
    "# your *.npy files.\n",
    "#\n",
    "npy_folder = '/content/drive/My Drive/Data/A1'     # on google drive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s-Y4MCnRsHg5"
   },
   "source": [
    "### Load Our Processed Data\n",
    "\n",
    "Here, write all the necessary codes to load the .npy files that we processed in Part 1 of our work into memory. Once done, reshape the data so that it can be fed into our Keras models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qW142dFNSVl6",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "input_size_0 = 44\n",
    "input_size_1 = 40\n",
    "\n",
    "# TODO:\n",
    "# Load the training and test data\n",
    "# NOTE: If you see the allow_pickle error when using Google Drive, just ignore \n",
    "#       it, and wait for a few minutes or re-upload the files again. \n",
    "#\n",
    "# ..................... CODES START HERE ..................... #\n",
    "\n",
    "xmfcc_tr= np.load(npy_folder + \"/x_mfcc_train.npy', encoding='bytes')\n",
    "xmfcc_te= np.load(npy_folder + \"/x_mfcc_test.npy', encoding='bytes')\n",
    "\n",
    "xspec_tr= np.load(npy_folder + \"/x_spec_train.npy', encoding='bytes')\n",
    "xspec_te= np.load(npy_folder + \"/x_spec_test.npy', encoding='bytes')\n",
    "# ...................... CODES END HERE ...................... #\n",
    "\n",
    "\n",
    "# TODO:\n",
    "# Reshape the x_spec arrays to (n, w, h, 1) and\n",
    "# reshape the x_mfcc arrays to (n, w, h, 1) and\n",
    "def reshape_x(x):\n",
    "    return x.reshape((x.shape[0], x.shape[1], x.shape[2], 1))\n",
    "\n",
    "# ..................... CODES START HERE ..................... #\n",
    "x_spec_train=reshape_x(xspec_tr)\n",
    "x_spec_test=reshape_x(xspec_te)\n",
    "\n",
    "x_mfcc_train=reshape_x(xmfcc_tr)\n",
    "x_mfcc_test=reshape_x(xmfcc_te)\n",
    "\n",
    "# ..................... CODES START HERE ..................... #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 755,
     "status": "ok",
     "timestamp": 1579508928887,
     "user": {
      "displayName": "Kim Foong Chow",
      "photoUrl": "",
      "userId": "11856562233682856896"
     },
     "user_tz": -480
    },
    "id": "7OOJ7XzmBAv7",
    "outputId": "52e309e7-0c9f-4bd9-cb45-44d51baa5fec"
   },
   "outputs": [],
   "source": [
    "print (x_spec_train.shape)\n",
    "print (x_spec_test.shape)\n",
    "print (x_mfcc_train.shape)\n",
    "print (x_mfcc_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uxfxU9dg7A2b"
   },
   "source": [
    "### Begin Training Our Different Convolutional Networks\n",
    "\n",
    "Here are going to create 2 different Convolutional Neural Network models. One model will predict using the spectrogram as the input. The second model will predict using the MFCC as the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bg8FuHPs3TqV"
   },
   "source": [
    "### CNN for Spectrogram \n",
    "\n",
    "Let's design the CNN model. For a start, you can use the following design:\n",
    "1. 2D Convolutional Layer, n1 3x3 filters\n",
    "2. 2D Max Pooling 2x2\n",
    "3. 2D Convolutional Layer, n2 3x3 filters\n",
    "4. 2D Max Pooling 2x2\n",
    "5. 2D Convolutional Layer, n3 3x3 filters\n",
    "6. 2D Max Pooling 2x2\n",
    "7. Dropout (0.0 - 1.0)\n",
    "8. 2D Global Max Pooling\n",
    "9. Dense (10 classes)\n",
    "\n",
    "NOTE: You are free to add or remove layers as long as the validation accuracy is reasonable. A reasonable model should yield about >75% validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 97704,
     "status": "ok",
     "timestamp": 1579509031786,
     "user": {
      "displayName": "Kim Foong Chow",
      "photoUrl": "",
      "userId": "11856562233682856896"
     },
     "user_tz": -480
    },
    "id": "3g1QU_w9SrTk",
    "outputId": "92688ae6-423f-46ec-ba2a-721882d36fe6"
   },
   "outputs": [],
   "source": [
    "# In this ConvNet model, we will have a few Convolutional layers.\n",
    "#\n",
    "# Go ahead a modify the network structure to try to improve performance.\n",
    "#\n",
    "def create_spec_model():\n",
    "\n",
    "    # TODO:\n",
    "    # Define your best sequential model here.\n",
    "    #\n",
    "    # ..................... CODES START HERE ..................... #\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', input_shape=(input_size_0, input_size_1, 1)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(256, kernel_size=(3, 3), activation='relu'))model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(GlobalMaxPooling2D())\n",
    " \n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    # ...................... CODES END HERE ...................... #\n",
    "    \n",
    "\n",
    "# Create the training folder\n",
    "#\n",
    "training_session_id = datetime.now().strftime(\"%Y-%m-%d %H-%M-%S\")\n",
    "training_session_folder = npy_folder + '/a1_spec_train_%s' % (training_session_id)\n",
    "os.makedirs(training_session_folder, exist_ok=True)\n",
    "print (training_session_folder)\n",
    "\n",
    "\n",
    "# Create the model and compile it.\n",
    "#\n",
    "spec_model = create_spec_model()\n",
    "spec_model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Configure the checkpoint and stop point.\n",
    "# This allows the training to save the best models and also stop the\n",
    "# training early if it detects that there are no improvements after\n",
    "# a long time.\n",
    "#\n",
    "callbacks_list = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=training_session_folder + '/a1_spec_model.{epoch:04d}-acc-{acc:4.2f}-val_acc-{val_acc:4.2f}-loss-{val_loss:4.2f}.h5',\n",
    "        monitor='val_loss', save_best_only=True),\n",
    "    keras.callbacks.EarlyStopping(monitor='val_loss', patience=100)\n",
    "]\n",
    "\n",
    "# Start training!\n",
    "#\n",
    "history_spec = spec_model.fit(x_spec_train, y_train, epochs=500, verbose=True, validation_data=(x_spec_test, y_test), callbacks=callbacks_list, batch_size=10)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Inu7HzPA4T7W"
   },
   "source": [
    "### CNN for MFCC \n",
    "\n",
    "Let's design the CNN model. For a start, you can use the following design:\n",
    "1. 2D Convolutional Layer, n1 3x3 filters\n",
    "2. 2D Max Pooling 2x2\n",
    "3. 2D Convolutional Layer, n2 3x3 filters\n",
    "4. 2D Max Pooling 2x2\n",
    "5. 2D Convolutional Layer, n3 3x3 filters\n",
    "6. 2D Max Pooling 2x2\n",
    "7. Dropout (0.0 - 1.0)\n",
    "8. 2D Global Max Pooling\n",
    "9. Dense (10 classes)\n",
    "\n",
    "NOTE: You are free to add or remove layers as long as the validation accuracy is reasonable. A reasonable model should yield >75% validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 62302,
     "status": "ok",
     "timestamp": 1579509099715,
     "user": {
      "displayName": "Kim Foong Chow",
      "photoUrl": "",
      "userId": "11856562233682856896"
     },
     "user_tz": -480
    },
    "id": "-12DX7E_kRlm",
    "outputId": "0131aeec-7c4e-4a54-8d1f-ab2fdcf1fa4f"
   },
   "outputs": [],
   "source": [
    "def create_mfcc_model():\n",
    "    # TODO:\n",
    "    # Design your best sequential model for MFCC here\n",
    "    #\n",
    "    # ..................... CODES START HERE ..................... #\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', input_shape=(input_size_0, input_size_1, 1)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(256, kernel_size=(3, 3), activation='relu'))model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(GlobalMaxPooling2D())\n",
    " \n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "    # ...................... CODES END HERE ...................... #\n",
    "\n",
    "\n",
    "# Create the training folder\n",
    "#\n",
    "training_session_id = datetime.now().strftime(\"%Y-%m-%d %H-%M-%S\")\n",
    "training_session_folder = npy_folder + '/a1_mfcc_train_%s' % (training_session_id)\n",
    "os.makedirs(training_session_folder, exist_ok=True)\n",
    "print (training_session_folder)\n",
    "\n",
    "\n",
    "# Create the model and compile it.\n",
    "#\n",
    "mfcc_model = create_mfcc_model()\n",
    "mfcc_model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Configure the checkpoint and stop point.\n",
    "# This allows the training to save the best models and also stop the\n",
    "# training early if it detects that there are no improvements after\n",
    "# a long time.\n",
    "#\n",
    "callbacks_list = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=training_session_folder + '/a1_mfcc_model.{epoch:04d}-acc-{acc:4.2f}-val_acc-{val_acc:4.2f}-loss-{val_loss:4.2f}.h5',\n",
    "        monitor='val_loss', save_best_only=True),\n",
    "    keras.callbacks.EarlyStopping(monitor='val_loss', patience=100)\n",
    "]\n",
    "\n",
    "history_mfcc = mfcc_model.fit(x_mfcc_train, y_train, epochs=500, verbose=True, validation_data=(x_mfcc_test, y_test), callbacks=callbacks_list, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r3WRwH2G3oKe"
   },
   "source": [
    "### Evaluate Our Model\n",
    "\n",
    "Let's take a look at your training process and see how well our model has performed on the both training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rt1nC4QUFKrE"
   },
   "outputs": [],
   "source": [
    "# Let's first initialize the labels for visualization\n",
    "#\n",
    "labels = [\"chainsaw\", \"clock_tick\", \"crackling_fire\", \"crying_baby\", \"dog\", \"helicopter\", \"rain\", \"rooster\", \"sea_waves\", \"sneezing\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GAXPmS3SGbim"
   },
   "source": [
    "Let's load up our best model to evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 16509,
     "status": "ok",
     "timestamp": 1579509391044,
     "user": {
      "displayName": "Kim Foong Chow",
      "photoUrl": "",
      "userId": "11856562233682856896"
     },
     "user_tz": -480
    },
    "id": "M7p--bU9GbIz",
    "outputId": "562d133e-39b3-4a24-88a1-0919728e617c"
   },
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# Change the name of the best performing h5 file to load it up.\n",
    "#\n",
    "mfcc_model = keras.models.load_model(npy_folder + \"/.../a1_mfcc_model....h5\")\n",
    "spec_model = keras.models.load_model(npy_folder + \"/.../a1_spec_model....h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i7F-DaWAb6M0"
   },
   "source": [
    "Then, run the following code, as is, to evaluate the full performance of your training and test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9049,
     "status": "ok",
     "timestamp": 1579509437103,
     "user": {
      "displayName": "Kim Foong Chow",
      "photoUrl": "",
      "userId": "11856562233682856896"
     },
     "user_tz": -480
    },
    "id": "IPOuXm4pLb7f",
    "outputId": "e58f2207-010e-463e-b7a6-33aaa3b9c591"
   },
   "outputs": [],
   "source": [
    "# Run this only if you need to use Google Drive\n",
    "\n",
    "#------------------------------------------------------------------------------------------\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# This function is used to display the history the train/test accuracy/loss\n",
    "# of the Keras training.\n",
    "#\n",
    "#   history - Pass in the history returned from the model.fit(...) method.\n",
    "#\n",
    "def display_training_loss_and_accuracy(history):\n",
    "    \n",
    "    plt.figure(figsize=(20,4))\n",
    "    \n",
    "    # summarize history for accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    \n",
    "    # summarize history for loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper right')\n",
    "    plt.show()    \n",
    "\n",
    "print (\"Training #1 - Spectrogram model\")\n",
    "display_training_loss_and_accuracy(history_spec)\n",
    "print (\"Training #2 - MFCC model\")\n",
    "display_training_loss_and_accuracy(history_mfcc)\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------------------\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "\n",
    "def plot_confusion_matrix(actual_y, y, plot_title, plot_nrows, plot_ncols, plot_index, labels, mask=None):\n",
    "    y_index = y.argmax(axis=1)\n",
    "    actual_y_index = actual_y.argmax(axis=1)\n",
    "\n",
    "    labels = np.array(labels)\n",
    "    if mask is not None:\n",
    "        labels_masked = labels[mask]\n",
    "    else:\n",
    "        labels_masked = labels\n",
    "    \n",
    "    # Print the first Confusion Matrix for the training data\n",
    "    #\n",
    "    cm = confusion_matrix(y_index, actual_y_index)\n",
    "    if mask is not None:\n",
    "        cm = cm[:, mask][mask, :]\n",
    "\n",
    "    cm_df = pd.DataFrame(cm, labels_masked, labels_masked)          \n",
    "    plt.subplot(plot_nrows, plot_ncols, plot_index)\n",
    "    plt.title(plot_title)\n",
    "    sns.heatmap(cm_df, annot=True)\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')   \n",
    "\n",
    " \n",
    "def display_classification_confusion_matrix(keras_model_list, x_train_list, y_train, x_test_list, y_test, labels, mask=None):\n",
    "\n",
    "    # Get predictions for all models\n",
    "    actual_y_train_list = [model.predict(x_train_list[i]) for i,model in enumerate(keras_model_list)]\n",
    "\n",
    "    # Ensemble by averaging the prediction from all models\n",
    "    actual_y_train = reduce(lambda a, b: a + b, actual_y_train_list) / len(actual_y_train_list)\n",
    "\n",
    "    # Get predictions for all models\n",
    "    actual_y_test_list = [model.predict(x_test_list[i]) for i,model in enumerate(keras_model_list)]\n",
    "\n",
    "    # Ensemble by averaging the prediction from all models\n",
    "    actual_y_test = reduce(lambda a, b: a + b, actual_y_test_list) / len(actual_y_test_list)\n",
    "\n",
    "     \n",
    "\n",
    "    for i in range(0, len(keras_model_list)):\n",
    "        print (\"Model %d\" % (i))\n",
    "        plt.figure(figsize=(20,5)) \n",
    "        plot_confusion_matrix(actual_y_train_list[i], y_train, \"Model %d (Train)\" % (i), 1, 2, 1, labels, mask)\n",
    "        plot_confusion_matrix(actual_y_test_list[i], y_test, \"Model %d (Test)\" % (i), 1, 2, 2, labels, mask)\n",
    "        plt.show()   \n",
    "\n",
    "        print (\"Train Data:\")\n",
    "        print(classification_report(actual_y_train_list[i].argmax(axis = 1), y_train.argmax(axis = 1), target_names=labels))\n",
    "        print (\"Test Data:\")\n",
    "        print(classification_report(actual_y_test_list[i].argmax(axis = 1), y_test.argmax(axis = 1), target_names=labels))\n",
    "        print (\"--------------------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "    print (\"Ensemble\")\n",
    "    plt.figure(figsize=(20,5)) \n",
    "    plot_confusion_matrix(actual_y_train, y_train, \"Ensemble (Train)\", 1, 2, 1, labels, mask)\n",
    "    plot_confusion_matrix(actual_y_test, y_test, \"Ensemble (Test)\", 1, 2, 2, labels, mask)\n",
    "    plt.show()   \n",
    "\n",
    "    print (\"Train Data:\")\n",
    "    print(classification_report(actual_y_train.argmax(axis = 1), y_train.argmax(axis = 1), target_names=labels))\n",
    "    print (\"Test Data:\")\n",
    "    print(classification_report(actual_y_test.argmax(axis = 1), y_test.argmax(axis = 1), target_names=labels))\n",
    "\n",
    "    \n",
    "\n",
    "# Display confusion matrix for all models\n",
    "#\n",
    "display_classification_confusion_matrix([spec_model, mfcc_model], [x_spec_train, x_mfcc_train], y_train, [x_spec_test, x_mfcc_test], y_test, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aC82EdnRmU1M"
   },
   "source": [
    "Ensure that the ensemble results perform better than the individual MFCC and Spectral models.\n",
    "\n",
    "Proceed to download both of your best .h5 models and incorporate it into Part 3 of the assignment.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "iti108-assg1-part2-solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
